{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "orR0KgfwrVjs"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk import FreqDist\n",
        "from collections import defaultdict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "import random\n",
        "\n",
        "# Download the Reuters corpus if it's not already available\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load and shuffle sentences from Reuters corpus\n",
        "sentences = list(reuters.sents())\n",
        "random.shuffle(sentences)\n",
        "\n",
        "# Continue with your existing code for corpus selection and model building\n",
        "# Example: Selecting 500â€“1000 sentences\n",
        "selected_sentences = sentences[:1000]\n",
        "\n",
        "# Display first few sentences to verify loading\n",
        "print(selected_sentences[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R3yNFnUojir",
        "outputId": "90d827dd-e10a-4701-c76f-cbb0086a04f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Around', '300', ',', '000', 'tonnes', 'were', 'exported', 'to', 'China', 'between', 'July', '1986', 'and', 'February', 'this', 'year', '.'], ['At', 'completion', 'of', 'this', 'transaction', ',', 'Northwest', 'Federal', 'would', 'be', 'a', 'subsidiary', 'of', 'LSB', \"'\", 's', 'non', '-', 'consolidated', 'wholly', '-', 'owned', 'financial', 'subsidiary', '.'], ['Although', 'the', 'World', 'Bank', 'had', 'pioneered', 'the', 'swap', 'market', ',', 'it', 'did', 'not', 'intend', 'to', 'launch', 'new', 'financial', 'instruments', 'just', 'for', 'the', 'sake', 'of', 'innovation', ',', 'Rotberg', 'said', '.'], ['\"', 'We', 'maintain', 'what', \"'\", 's', 'going', 'to', 'drive', 'Canada', \"'\", 's', 'export', 'performance', 'is', 'income', 'growth', 'in', 'the', 'U', '.', 'S', '.,\"', 'said', 'Midland', \"'\", 's', 'Donegan', '.'], ['The', 'initial', 'rate', 'was', 'also', 'to', 'be', 'initially', 'set', 'on', 'April', '15', ',', 'and', 'quarterly', 'after', 'that', ',', 'at', '340', 'basis', 'points', 'below', 'the', 'same', '90', '-', 'day', 'New', 'Zealand', 'rate', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and shuffle sentences from Reuters corpus\n",
        "sentences = list(reuters.sents())\n",
        "random.shuffle(sentences)\n"
      ],
      "metadata": {
        "id": "qbfFhBXJvZhK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 500-1000 sentences\n",
        "selected_sentences = sentences[:1000]"
      ],
      "metadata": {
        "id": "Q9pAUop020J5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the list of words in each sentence to a single corpus text\n",
        "corpus = ' '.join([' '.join(sent) for sent in selected_sentences])\n",
        "tokens = nltk.word_tokenize(corpus.lower())"
      ],
      "metadata": {
        "id": "9eRCEobauCiU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "wwLYY6FGpgc5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation from tokens for unigram model\n",
        "tokens = [word for word in tokens if word not in string.punctuation]"
      ],
      "metadata": {
        "id": "DMO9D0tX6HPp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build unigram, bigram, and trigram frequency distributions\n",
        "unigrams = FreqDist(tokens)\n",
        "bigrams = FreqDist(ngrams(tokens, 2))\n",
        "trigrams = FreqDist(ngrams(tokens, 3))\n"
      ],
      "metadata": {
        "id": "36-y0QXuuHiV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert frequency distributions to dictionaries for easier access\n",
        "unigram_model = dict(unigrams)\n",
        "bigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "trigram_model = defaultdict(lambda: defaultdict(lambda: 0))"
      ],
      "metadata": {
        "id": "WsVjI4kfuNG7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate bigram and trigram models\n",
        "for (w1, w2), freq in bigrams.items():\n",
        "    bigram_model[w1][w2] = freq\n",
        "for (w1, w2, w3), freq in trigrams.items():\n",
        "    trigram_model[(w1, w2)][w3] = freq"
      ],
      "metadata": {
        "id": "_Dp6sYLKunDS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the next word prediction\n",
        "def predict_next_word(sequence, model=\"unigram\"):\n",
        "    sequence = sequence.lower().split()\n",
        "    if model == \"unigram\":\n",
        "        return max(unigram_model, key=unigram_model.get)\n",
        "    elif model == \"bigram\" and len(sequence) >= 1:\n",
        "        w1 = sequence[-1]\n",
        "        if w1 in bigram_model:\n",
        "            return max(bigram_model[w1], key=bigram_model[w1].get)\n",
        "    elif model == \"trigram\" and len(sequence) >= 2:\n",
        "        w1, w2 = sequence[-2], sequence[-1]\n",
        "        if (w1, w2) in trigram_model:\n",
        "            return max(trigram_model[(w1, w2)], key=trigram_model[(w1, w2)].get)\n",
        "    return None  # Return None if no prediction is possibl"
      ],
      "metadata": {
        "id": "AahWOB9duumt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a sample sequence of words\n",
        "def create_sample_sequence(sentence, length=3):\n",
        "    if len(sentence) >= length:\n",
        "        start = random.randint(0, len(sentence) - length)  # Random start position\n",
        "        return ' '.join(sentence[start:start + length])\n",
        "    return ' '.join(sentence)"
      ],
      "metadata": {
        "id": "ZnFdpta33goX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure selected_sentences is a list\n",
        "selected_sentences = list(selected_sentences)\n",
        "\n",
        "# Generate 10 sample sequences from Reuters corpus\n",
        "sample_sequences = [create_sample_sequence(sentence) for sentence in random.sample(selected_sentences, 10)]\n"
      ],
      "metadata": {
        "id": "QNWTClGJu-Rj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print and fill table with predictions\n",
        "print(\"Sr.\\tSequence of Words\\tUni-Gram\\tBi-Gram\\tTri-Gram\")\n",
        "for i, sequence in enumerate(sample_sequences, 1):\n",
        "    unigram_pred = predict_next_word(sequence, model=\"unigram\")\n",
        "    bigram_pred = predict_next_word(sequence, model=\"bigram\")\n",
        "    trigram_pred = predict_next_word(sequence, model=\"trigram\")\n",
        "    print(f\"{i}\\t{sequence}\\t{unigram_pred}    \\t{bigram_pred}     \\t{trigram_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OACU7kMdvE-y",
        "outputId": "d3d3a272-31bb-4642-b51e-6f9b2c54bb04"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sr.\tSequence of Words\tUni-Gram\tBi-Gram\tTri-Gram\n",
            "1\tnew price for\tthe    \tthe     \thigh\n",
            "2\tKeidanren ) said\tthe    \tthe     \tNone\n",
            "3\tcould have a\tthe    \tshare     \tprofound\n",
            "4\tstate oil company\tthe    \tsaid     \tpertamina\n",
            "5\texploration program ,\tthe    \tNone     \tNone\n",
            "6\t554 dlrs for\tthe    \tthe     \tthe\n",
            "7\tMarch Eight Shr\tthe    \tloss     \tloss\n",
            "8\tan agreement with\tthe    \tthe     \tthe\n",
            "9\tfor injured workers\tthe    \tat     \tamerican\n",
            "10\t- day visit\tthe    \tto     \tto\n"
          ]
        }
      ]
    }
  ]
}